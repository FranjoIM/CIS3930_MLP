{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d773866",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pyreadr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ba3fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getData(data):\n",
    "  return pyreadr.read_r(data)[None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17cf6372",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df = getData(\"Batch.rds\")\n",
    "genetic_df = getData(\"Genetic.rds\")\n",
    "ocd_df = getData(\"OCD.rds\")\n",
    "que_df = getData(\"QUE.rds\")\n",
    "dmri_df  = getData(\"dMRI.rds\")\n",
    "smri_df  = getData(\"sMRI.rds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665bd224",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df.head()\n",
    "batch_df['OCD']=batch_df['OCD'].fillna(1)\n",
    "batch_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3219c1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "genetic_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56302e45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ocd_df.head()\n",
    "ocd_df['OCD']=ocd_df['OCD'].fillna(1)\n",
    "ocd_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302c9feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "que_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20033c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmri_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800530af",
   "metadata": {},
   "outputs": [],
   "source": [
    "smri_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f37c99",
   "metadata": {},
   "source": [
    "### QUESTIONNAIRE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f9a2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_coef = []\n",
    "train_intercept = []\n",
    "for i in range(1,77):\n",
    "  sample_ids = list(batch_df.loc[batch_df['Train_'+str(i)]==1]['SampleID'])\n",
    "  que_np = que_df.loc[que_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:]\n",
    "  que_np=que_np.astype('float64')\n",
    "  np.isnan(que_np)\n",
    "  que_np[np.isnan(que_np)]=1.0\n",
    "  ocd_np = np.concatenate(ocd_df.loc[ocd_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:])\n",
    "  ocd_np[ocd_np==2]=1\n",
    "  ocd_np = ocd_np.astype('int')\n",
    "  model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.8, max_iter=5000).fit(que_np,ocd_np)\n",
    "  train_coef.append(model.coef_)\n",
    "  train_intercept.append(model.intercept_)\n",
    "  print(\"Progress ===> \",i,\" model completed\")\n",
    "print(train_coef)\n",
    "print(train_intercept)\n",
    "train_coef = np.concatenate(train_coef)\n",
    "train_intercept = np.concatenate(train_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5146741",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "train_model_coef = np.mean(train_coef, axis=0)\n",
    "train_model_intercept = np.mean(train_intercept)\n",
    "\n",
    "sample_ids = list(batch_df.loc[batch_df['Valid']==1]['SampleID'])\n",
    "que_train_np = que_df.loc[que_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:]\n",
    "que_train_np = que_train_np.astype('float64')\n",
    "np.isnan(que_train_np)\n",
    "que_train_np[np.isnan(que_train_np)]=1.0\n",
    "ocd_train_np = np.concatenate(ocd_df.loc[ocd_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:])\n",
    "ocd_train_np[ocd_train_np==2]=1\n",
    "train_y_prob=[]\n",
    "for i in range(len(que_train_np)):\n",
    "  train_y_prob.append(1/(1+np.exp(-(np.dot(que_train_np[i],train_model_coef)+train_model_intercept))))\n",
    "train_y_pred=np.array(train_y_prob)\n",
    "train_y_pred[train_y_pred>0.5]=1\n",
    "train_y_pred[train_y_pred<=0.5]=0\n",
    "train_y_pred = train_y_pred.astype('int64')\n",
    "ocd_train_np = ocd_train_np.astype('int64')\n",
    "print(accuracy_score(ocd_train_np,train_y_pred))\n",
    "print(confusion_matrix(ocd_train_np,train_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b73bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "TN, FP, FN, TP = confusion_matrix(ocd_train_np,train_y_pred).ravel()\n",
    "\n",
    "print(confusion_matrix(ocd_train_np, train_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc5eb318",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "P = TP + FN\n",
    "N = FP + TN\n",
    "PP = TP + FP\n",
    "PN = FN + TN\n",
    "\n",
    "print(\"Population =\", P+N)\n",
    "print(\"Prevalence =\", P/(P+N))\n",
    "\n",
    "CK = (2*((TP * TN) - (TN * FP)))/(((TP + FP) * (FP + TN)) + ((TP + FN) * (FN+TN)))\n",
    "ACC = (TP + TN) / (P + N)\n",
    "PPV = TP / PP    \n",
    "FOR = FN / PN\n",
    "FDR = FP / PP\n",
    "NPV = TN / PN\n",
    "TPR = TP / P\n",
    "FPR = FP / N\n",
    "FNR = FN / P\n",
    "TNR = TN / N\n",
    "LRp = TPR / FPR\n",
    "LRn = FNR / TNR\n",
    "MK = PPV + NPV - 1\n",
    "BM = TPR + TNR - 1\n",
    "PT = (sqrt(TPR+FPR) - FPR) / (TPR - FPR)\n",
    "DOR = LRp/LRn\n",
    "BA = (TPR + TNR) / 2\n",
    "FS = (2*PPV * TPR) / (PPV + TPR)\n",
    "FM = sqrt(PPV * TPR)\n",
    "MCC = sqrt(TPR*TNR*PPV*NPV) - sqrt(FPR*FNR*FDR*FOR)\n",
    "TS = TP / (TP + FN + FP)\n",
    "       \n",
    "print(CK)\n",
    "print(ACC)\n",
    "print(PPV)\n",
    "print(FOR)\n",
    "print(FDR)\n",
    "print(NPV)\n",
    "print(TPR)\n",
    "print(FPR)\n",
    "print(TNR)\n",
    "print(FNR)\n",
    "print(LRp)\n",
    "print(LRn)\n",
    "print(MK)\n",
    "print(BM)\n",
    "print(PT)\n",
    "print(DOR)\n",
    "print(BA)\n",
    "print(FS)\n",
    "print(FM)\n",
    "print(MCC)\n",
    "print(TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48551a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('que_coef.npy', 'wb') as f:\n",
    "    np.save(f, train_coef)\n",
    "with open('que_intercept.npy', 'wb') as f:\n",
    "    np.save(f, train_intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9b70b5",
   "metadata": {},
   "source": [
    "### dMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af2d062d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_coef = []\n",
    "train_intercept = []\n",
    "for i in range(1,77):\n",
    "  sample_ids = list(batch_df.loc[batch_df['Train_'+str(i)]==1]['SampleID'])\n",
    "  dmri_np = dmri_df.loc[dmri_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:]\n",
    "  dmri_np=dmri_np.astype('float64')\n",
    "  dmri_np[np.isnan(dmri_np)]=1.0\n",
    "  ocd_np = np.concatenate(ocd_df.loc[ocd_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:])\n",
    "  ocd_np[ocd_np==2]=1\n",
    "  ocd_np = ocd_np.astype('int')\n",
    "  model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.8, max_iter=5000).fit(dmri_np,ocd_np)\n",
    "  train_coef.append(model.coef_)\n",
    "  train_intercept.append(model.intercept_)\n",
    "  print(\"Progress ===> \",i,\" model completed\")\n",
    "print(train_coef)\n",
    "print(train_intercept)\n",
    "train_coef = np.concatenate(train_coef)\n",
    "train_intercept = np.concatenate(train_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8857181e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "train_model_coef = np.mean(train_coef, axis=0)\n",
    "train_model_intercept = np.mean(train_intercept)\n",
    "\n",
    "sample_ids = list(batch_df.loc[batch_df['Valid']==1]['SampleID'])\n",
    "dmri_train_np = dmri_df.loc[dmri_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:]\n",
    "dmri_train_np = dmri_train_np.astype('float64')\n",
    "np.isnan(dmri_train_np)\n",
    "dmri_train_np[np.isnan(dmri_train_np)]=1.0\n",
    "ocd_train_np = np.concatenate(ocd_df.loc[ocd_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:])\n",
    "ocd_train_np[ocd_train_np==2]=1\n",
    "train_y_prob=[]\n",
    "for i in range(len(que_train_np)):\n",
    "  train_y_prob.append(1/(1+np.exp(-(np.dot(dmri_train_np[i],train_model_coef)+train_model_intercept))))\n",
    "train_y_pred=np.array(train_y_prob)\n",
    "train_y_pred[train_y_pred>0.5]=1\n",
    "train_y_pred[train_y_pred<=0.5]=0\n",
    "train_y_pred = train_y_pred.astype('int64')\n",
    "ocd_train_np = ocd_train_np.astype('int64')\n",
    "print(accuracy_score(ocd_train_np,train_y_pred))\n",
    "print(confusion_matrix(ocd_train_np,train_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4951515d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "TN, FP, FN, TP = confusion_matrix(ocd_train_np,train_y_pred).ravel()\n",
    "\n",
    "print(confusion_matrix(ocd_train_np, train_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef02407d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "P = TP + FN\n",
    "N = FP + TN\n",
    "PP = TP + FP\n",
    "PN = FN + TN\n",
    "\n",
    "print(\"Population =\", P+N)\n",
    "print(\"Prevalence =\", P/(P+N))\n",
    "\n",
    "CK = (2*((TP * TN) - (TN * FP)))/(((TP + FP) * (FP + TN)) + ((TP + FN) * (FN+TN)))\n",
    "ACC = (TP + TN) / (P + N)\n",
    "PPV = TP / PP    \n",
    "FOR = FN / PN\n",
    "FDR = FP / PP\n",
    "NPV = TN / PN\n",
    "TPR = TP / P\n",
    "FPR = FP / N\n",
    "FNR = FN / P\n",
    "TNR = TN / N\n",
    "LRp = TPR / FPR\n",
    "LRn = FNR / TNR\n",
    "MK = PPV + NPV - 1\n",
    "BM = TPR + TNR - 1\n",
    "PT = (sqrt(TPR+FPR) - FPR) / (TPR - FPR)\n",
    "DOR = LRp/LRn\n",
    "BA = (TPR + TNR) / 2\n",
    "FS = (2*PPV * TPR) / (PPV + TPR)\n",
    "FM = sqrt(PPV * TPR)\n",
    "MCC = sqrt(TPR*TNR*PPV*NPV) - sqrt(FPR*FNR*FDR*FOR)\n",
    "TS = TP / (TP + FN + FP)\n",
    "       \n",
    "print(CK)\n",
    "print(ACC)\n",
    "print(PPV)\n",
    "print(FOR)\n",
    "print(FDR)\n",
    "print(NPV)\n",
    "print(TPR)\n",
    "print(FPR)\n",
    "print(TNR)\n",
    "print(FNR)\n",
    "print(LRp)\n",
    "print(LRn)\n",
    "print(MK)\n",
    "print(BM)\n",
    "print(PT)\n",
    "print(DOR)\n",
    "print(BA)\n",
    "print(FS)\n",
    "print(FM)\n",
    "print(MCC)\n",
    "print(TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba0aecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dMRI_coef.npy', 'wb') as f:\n",
    "    np.save(f, train_coef)\n",
    "with open('dMRI_intercept.npy', 'wb') as f:\n",
    "    np.save(f, train_intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fadfde3",
   "metadata": {},
   "source": [
    "### sMRI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ee61c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_coef = []\n",
    "train_intercept = []\n",
    "for i in range(1,77):\n",
    "  sample_ids = list(batch_df.loc[batch_df['Train_'+str(i)]==1]['SampleID'])\n",
    "  smri_np = smri_df.loc[smri_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:]\n",
    "  smri_np = smri_np.astype('float64')\n",
    "  smri_np[np.isnan(smri_np)]=1.0\n",
    "  ocd_np = np.concatenate(ocd_df.loc[ocd_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:])\n",
    "  ocd_np[ocd_np==2]=1\n",
    "  ocd_np = ocd_np.astype('int')\n",
    "  model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.8, max_iter=5000).fit(smri_np,ocd_np)\n",
    "  train_coef.append(model.coef_)\n",
    "  train_intercept.append(model.intercept_)\n",
    "  print(\"Progress ===> \",i,\" model completed\")\n",
    "print(train_coef)\n",
    "print(train_intercept)\n",
    "train_coef = np.concatenate(train_coef)\n",
    "train_intercept = np.concatenate(train_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43720424",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "train_model_coef = np.mean(train_coef, axis=0)\n",
    "train_model_intercept = np.mean(train_intercept)\n",
    "\n",
    "sample_ids = list(batch_df.loc[batch_df['Valid']==1]['SampleID'])\n",
    "smri_train_np = smri_df.loc[smri_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:]\n",
    "smri_train_np = smri_train_np.astype('float64')\n",
    "np.isnan(smri_train_np)\n",
    "smri_train_np[np.isnan(smri_train_np)]=1.0\n",
    "ocd_train_np = np.concatenate(ocd_df.loc[ocd_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:])\n",
    "ocd_train_np[ocd_train_np==2]=1\n",
    "train_y_prob=[]\n",
    "for i in range(len(que_train_np)):\n",
    "  train_y_prob.append(1/(1+np.exp(-(np.dot(smri_train_np[i],train_model_coef)+train_model_intercept))))\n",
    "train_y_pred=np.array(train_y_prob)\n",
    "train_y_pred[train_y_pred>0.5]=1\n",
    "train_y_pred[train_y_pred<=0.5]=0\n",
    "train_y_pred = train_y_pred.astype('int64')\n",
    "ocd_train_np = ocd_train_np.astype('int64')\n",
    "print(accuracy_score(ocd_train_np,train_y_pred))\n",
    "print(confusion_matrix(ocd_train_np,train_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e707a89c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "TN, FP, FN, TP = confusion_matrix(ocd_train_np,train_y_pred).ravel()\n",
    "\n",
    "print(confusion_matrix(ocd_train_np, train_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15cc7957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "P = TP + FN\n",
    "N = FP + TN\n",
    "PP = TP + FP\n",
    "PN = FN + TN\n",
    "\n",
    "print(\"Population =\", P+N)\n",
    "print(\"Prevalence =\", P/(P+N))\n",
    "\n",
    "CK = (2*((TP * TN) - (TN * FP)))/(((TP + FP) * (FP + TN)) + ((TP + FN) * (FN+TN)))\n",
    "ACC = (TP + TN) / (P + N)\n",
    "PPV = TP / PP    \n",
    "FOR = FN / PN\n",
    "FDR = FP / PP\n",
    "NPV = TN / PN\n",
    "TPR = TP / P\n",
    "FPR = FP / N\n",
    "FNR = FN / P\n",
    "TNR = TN / N\n",
    "LRp = TPR / FPR\n",
    "LRn = FNR / TNR\n",
    "MK = PPV + NPV - 1\n",
    "BM = TPR + TNR - 1\n",
    "PT = (sqrt(TPR+FPR) - FPR) / (TPR - FPR)\n",
    "DOR = LRp/LRn\n",
    "BA = (TPR + TNR) / 2\n",
    "FS = (2*PPV * TPR) / (PPV + TPR)\n",
    "FM = sqrt(PPV * TPR)\n",
    "MCC = sqrt(TPR*TNR*PPV*NPV) - sqrt(FPR*FNR*FDR*FOR)\n",
    "TS = TP / (TP + FN + FP)\n",
    "       \n",
    "print(CK)\n",
    "print(ACC)\n",
    "print(PPV)\n",
    "print(FOR)\n",
    "print(FDR)\n",
    "print(NPV)\n",
    "print(TPR)\n",
    "print(FPR)\n",
    "print(TNR)\n",
    "print(FNR)\n",
    "print(LRp)\n",
    "print(LRn)\n",
    "print(MK)\n",
    "print(BM)\n",
    "print(PT)\n",
    "print(DOR)\n",
    "print(BA)\n",
    "print(FS)\n",
    "print(FM)\n",
    "print(MCC)\n",
    "print(TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e3ab41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sMRI_coef.npy', 'wb') as f:\n",
    "    np.save(f, train_coef)\n",
    "with open('sMRI_intercept.npy', 'wb') as f:\n",
    "    np.save(f, train_intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e35916e",
   "metadata": {},
   "source": [
    "### GENETICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b7a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import accuracy_score\n",
    "train_coef = []\n",
    "train_intercept = []\n",
    "for i in range(1,77):\n",
    "  sample_ids = list(batch_df.loc[batch_df['Train_'+str(i)]==1]['SampleID'])\n",
    "  genetic_np = genetic_df.loc[genetic_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:]\n",
    "  ocd_np = np.concatenate(ocd_df.loc[ocd_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:])\n",
    "  ocd_np[ocd_np==2]=1\n",
    "  ocd_np = ocd_np.astype('int')\n",
    "  model = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.8, max_iter=5000).fit(genetic_np,ocd_np)\n",
    "  train_coef.append(model.coef_)\n",
    "  train_intercept.append(model.intercept_)\n",
    "  print(\"Progress ===> \",i,\" model completed\")\n",
    "print(train_coef)\n",
    "print(train_intercept)\n",
    "train_coef = np.concatenate(train_coef)\n",
    "train_intercept = np.concatenate(train_intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "799ef038",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "train_model_coef = np.mean(train_coef, axis=0)\n",
    "train_model_intercept = np.mean(train_intercept)\n",
    "\n",
    "sample_ids = list(batch_df.loc[batch_df['Valid']==1]['SampleID'])\n",
    "genetic_train_np = genetic_df.loc[genetic_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:]\n",
    "ocd_train_np = np.concatenate(ocd_df.loc[ocd_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:])\n",
    "ocd_train_np[ocd_train_np==2]=1\n",
    "train_y_prob=[]\n",
    "for i in range(len(genetic_train_np)):\n",
    "  train_y_prob.append(1/(1+np.exp(-(np.dot(genetic_train_np[i],train_model_coef)+train_model_intercept))))\n",
    "train_y_pred=np.array(train_y_prob)\n",
    "train_y_pred[train_y_pred>0.5]=1\n",
    "train_y_pred[train_y_pred<=0.5]=0\n",
    "train_y_pred = train_y_pred.astype('int64')\n",
    "ocd_train_np = ocd_train_np.astype('int64')\n",
    "print(accuracy_score(ocd_train_np,train_y_pred))\n",
    "print(confusion_matrix(ocd_train_np,train_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0af55c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "TN, FP, FN, TP = confusion_matrix(ocd_train_np,train_y_pred).ravel()\n",
    "\n",
    "print(confusion_matrix(ocd_train_np, train_y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bee925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "P = TP + FN\n",
    "N = FP + TN\n",
    "PP = TP + FP\n",
    "PN = FN + TN\n",
    "\n",
    "print(\"Population =\", P+N)\n",
    "print(\"Prevalence =\", P/(P+N))\n",
    "\n",
    "CK = (2*((TP * TN) - (TN * FP)))/(((TP + FP) * (FP + TN)) + ((TP + FN) * (FN+TN)))\n",
    "ACC = (TP + TN) / (P + N)\n",
    "PPV = TP / PP    \n",
    "FOR = FN / PN\n",
    "FDR = FP / PP\n",
    "NPV = TN / PN\n",
    "TPR = TP / P\n",
    "FPR = FP / N\n",
    "FNR = FN / P\n",
    "TNR = TN / N\n",
    "LRp = TPR / FPR\n",
    "LRn = FNR / TNR\n",
    "MK = PPV + NPV - 1\n",
    "BM = TPR + TNR - 1\n",
    "PT = (sqrt(TPR+FPR) - FPR) / (TPR - FPR)\n",
    "DOR = LRp/LRn\n",
    "BA = (TPR + TNR) / 2\n",
    "FS = (2*PPV * TPR) / (PPV + TPR)\n",
    "FM = sqrt(PPV * TPR)\n",
    "MCC = sqrt(TPR*TNR*PPV*NPV) - sqrt(FPR*FNR*FDR*FOR)\n",
    "TS = TP / (TP + FN + FP)\n",
    "       \n",
    "print(CK)\n",
    "print(ACC)\n",
    "print(PPV)\n",
    "print(FOR)\n",
    "print(FDR)\n",
    "print(NPV)\n",
    "print(TPR)\n",
    "print(FPR)\n",
    "print(TNR)\n",
    "print(FNR)\n",
    "print(LRp)\n",
    "print(LRn)\n",
    "print(MK)\n",
    "print(BM)\n",
    "print(PT)\n",
    "print(DOR)\n",
    "print(BA)\n",
    "print(FS)\n",
    "print(FM)\n",
    "print(MCC)\n",
    "print(TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe20a763",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gen_coef.npy', 'wb') as f:\n",
    "    np.save(f, train_coef)\n",
    "with open('gen_intercept.npy', 'wb') as f:\n",
    "    np.save(f, train_intercept)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46215a07",
   "metadata": {},
   "source": [
    "### COMBINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cfdd13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "acc_gen = 0.61\n",
    "acc_smri = 0.61\n",
    "acc_dmri = 0.54\n",
    "acc_que = 0.45\n",
    "acc_tot = acc_gen + acc_smri + acc_dmri + acc_que\n",
    "\n",
    "w_gen = acc_gen / acc_tot\n",
    "w_smri = acc_smri / acc_tot\n",
    "w_dmri = acc_dmri / acc_tot\n",
    "w_que = acc_que / acc_tot\n",
    "\n",
    "print(w_gen, w_smri, w_dmri, w_que)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29a1a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import coefficients\n",
    "gen_coef = None\n",
    "gen_intercept = None\n",
    "que_coef = None\n",
    "que_intercept = None\n",
    "smri_coef = None\n",
    "smri_intercept = None\n",
    "dmri_coef = None\n",
    "dmri_intercept = None\n",
    "\n",
    "with open('gen_coef.npy', 'rb') as f:\n",
    "    gen_coef = np.load(f)\n",
    "with open('gen_intercept.npy', 'rb') as f:\n",
    "    gen_intercept = np.load(f)\n",
    "    \n",
    "with open('que_coef.npy', 'rb') as f:\n",
    "    que_coef = np.load(f)\n",
    "with open('que_intercept.npy', 'rb') as f:\n",
    "    que_intercept = np.load(f)\n",
    "\n",
    "with open('smri_coef.npy', 'rb') as f:\n",
    "    smri_coef = np.load(f)\n",
    "with open('smri_intercept.npy', 'rb') as f:\n",
    "    smri_intercept = np.load(f)\n",
    "    \n",
    "with open('dmri_coef.npy', 'rb') as f:\n",
    "    dmri_coef = np.load(f)\n",
    "with open('dmri_intercept.npy', 'rb') as f:\n",
    "    dmri_intercept = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af573a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# gen\n",
    "train_model_coef = np.mean(gen_coef, axis=0)\n",
    "train_model_intercept = np.mean(gen_intercept)\n",
    "\n",
    "sample_ids = list(batch_df.loc[batch_df['Test']==1]['SampleID'])\n",
    "genetic_train_np = genetic_df.loc[genetic_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:]\n",
    "ocd_train_np = np.concatenate(ocd_df.loc[ocd_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:])\n",
    "ocd_train_np[ocd_train_np==2]=1\n",
    "train_y_prob=[]\n",
    "for i in range(len(genetic_train_np)):\n",
    "  train_y_prob.append(1/(1+np.exp(-(np.dot(genetic_train_np[i],train_model_coef)+train_model_intercept))))\n",
    "gen_y_prob=np.array(train_y_prob)\n",
    "\n",
    "# sMRI\n",
    "train_model_coef = np.mean(smri_coef, axis=0)\n",
    "train_model_intercept = np.mean(smri_intercept)\n",
    "\n",
    "sample_ids = list(batch_df.loc[batch_df['Test']==1]['SampleID'])\n",
    "smri_train_np = smri_df.loc[smri_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:]\n",
    "smri_train_np = smri_train_np.astype('float64')\n",
    "np.isnan(smri_train_np)\n",
    "smri_train_np[np.isnan(smri_train_np)]=1.0\n",
    "ocd_train_np = np.concatenate(ocd_df.loc[ocd_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:])\n",
    "ocd_train_np[ocd_train_np==2]=1\n",
    "train_y_prob=[]\n",
    "for i in range(len(que_train_np)):\n",
    "  train_y_prob.append(1/(1+np.exp(-(np.dot(smri_train_np[i],train_model_coef)+train_model_intercept))))\n",
    "smri_y_prob=np.array(train_y_prob)\n",
    "\n",
    "# dMRI\n",
    "train_model_coef = np.mean(dmri_coef, axis=0)\n",
    "train_model_intercept = np.mean(dmri_intercept)\n",
    "\n",
    "sample_ids = list(batch_df.loc[batch_df['Test']==1]['SampleID'])\n",
    "dmri_train_np = dmri_df.loc[dmri_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:]\n",
    "dmri_train_np = dmri_train_np.astype('float64')\n",
    "np.isnan(dmri_train_np)\n",
    "dmri_train_np[np.isnan(dmri_train_np)]=1.0\n",
    "ocd_train_np = np.concatenate(ocd_df.loc[ocd_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:])\n",
    "ocd_train_np[ocd_train_np==2]=1\n",
    "train_y_prob=[]\n",
    "for i in range(len(que_train_np)):\n",
    "  train_y_prob.append(1/(1+np.exp(-(np.dot(dmri_train_np[i],train_model_coef)+train_model_intercept))))\n",
    "dmri_y_prob=np.array(train_y_prob)\n",
    "\n",
    "# que\n",
    "train_model_coef = np.mean(que_coef, axis=0)\n",
    "train_model_intercept = np.mean(que_intercept)\n",
    "\n",
    "sample_ids = list(batch_df.loc[batch_df['Test']==1]['SampleID'])\n",
    "que_train_np = que_df.loc[que_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:]\n",
    "que_train_np = que_train_np.astype('float64')\n",
    "np.isnan(que_train_np)\n",
    "que_train_np[np.isnan(que_train_np)]=1.0\n",
    "ocd_train_np = np.concatenate(ocd_df.loc[ocd_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:])\n",
    "ocd_train_np[ocd_train_np==2]=1\n",
    "train_y_prob=[]\n",
    "for i in range(len(que_train_np)):\n",
    "  train_y_prob.append(1/(1+np.exp(-(np.dot(que_train_np[i],train_model_coef)+train_model_intercept))))\n",
    "que_y_prob=np.array(train_y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeafbbbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Probabilities\n",
    "wgen_y_prob = gen_y_prob * w_gen\n",
    "wsmri_y_prob = smri_y_prob * w_smri\n",
    "wdmri_y_prob = dmri_y_prob * w_dmri\n",
    "wque_y_prob = que_y_prob * w_que\n",
    "\n",
    "total_y_prob = wgen_y_prob + wsmri_y_prob + wdmri_y_prob + wque_y_prob\n",
    "\n",
    "print(total_y_prob)\n",
    "\n",
    "total_y_prob[total_y_prob>0.5]=1\n",
    "total_y_prob[total_y_prob<=0.5]=0\n",
    "total_y_prob = total_y_prob.astype('int64')\n",
    "ocd_train_np = ocd_train_np.astype('int64')\n",
    "print(accuracy_score(ocd_train_np,total_y_prob))\n",
    "print(confusion_matrix(ocd_train_np,total_y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8744fc9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "TN, FP, FN, TP = confusion_matrix(ocd_train_np,total_y_prob).ravel()\n",
    "\n",
    "print(confusion_matrix(ocd_train_np, total_y_prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd95332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "P = TP + FN\n",
    "N = FP + TN\n",
    "PP = TP + FP\n",
    "PN = FN + TN\n",
    "\n",
    "print(\"Population =\", P+N)\n",
    "print(\"Prevalence =\", P/(P+N))\n",
    "\n",
    "CK = (2*((TP * TN) - (TN * FP)))/(((TP + FP) * (FP + TN)) + ((TP + FN) * (FN+TN)))\n",
    "ACC = (TP + TN) / (P + N)\n",
    "PPV = TP / PP    \n",
    "FOR = FN / PN\n",
    "FDR = FP / PP\n",
    "NPV = TN / PN\n",
    "TPR = TP / P\n",
    "FPR = FP / N\n",
    "FNR = FN / P\n",
    "TNR = TN / N\n",
    "LRp = TPR / FPR\n",
    "LRn = FNR / TNR\n",
    "MK = PPV + NPV - 1\n",
    "BM = TPR + TNR - 1\n",
    "PT = (sqrt(TPR+FPR) - FPR) / (TPR - FPR)\n",
    "DOR = LRp/LRn\n",
    "BA = (TPR + TNR) / 2\n",
    "FS = (2*PPV * TPR) / (PPV + TPR)\n",
    "FM = sqrt(PPV * TPR)\n",
    "MCC = sqrt(TPR*TNR*PPV*NPV) - sqrt(FPR*FNR*FDR*FOR)\n",
    "TS = TP / (TP + FN + FP)\n",
    "       \n",
    "print(CK)\n",
    "print(ACC)\n",
    "print(PPV)\n",
    "print(FOR)\n",
    "print(FDR)\n",
    "print(NPV)\n",
    "print(TPR)\n",
    "print(FPR)\n",
    "print(TNR)\n",
    "print(FNR)\n",
    "print(LRp)\n",
    "print(LRn)\n",
    "print(MK)\n",
    "print(BM)\n",
    "print(PT)\n",
    "print(DOR)\n",
    "print(BA)\n",
    "print(FS)\n",
    "print(FM)\n",
    "print(MCC)\n",
    "print(TS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cca9913",
   "metadata": {},
   "source": [
    "### AMBIGUOUS CASES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d505eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# gen\n",
    "train_model_coef = np.mean(gen_coef, axis=0)\n",
    "train_model_intercept = np.mean(gen_intercept)\n",
    "\n",
    "sample_ids = list(batch_df.loc[batch_df['Experimental']==1]['SampleID'])\n",
    "genetic_train_np = genetic_df.loc[genetic_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:]\n",
    "ocd_train_np = np.concatenate(ocd_df.loc[ocd_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:])\n",
    "train_y_prob=[]\n",
    "for i in range(len(genetic_train_np)):\n",
    "  train_y_prob.append(1/(1+np.exp(-(np.dot(genetic_train_np[i],train_model_coef)+train_model_intercept))))\n",
    "gen_y_prob=np.array(train_y_prob)\n",
    "\n",
    "# sMRI\n",
    "train_model_coef = np.mean(smri_coef, axis=0)\n",
    "train_model_intercept = np.mean(smri_intercept)\n",
    "\n",
    "sample_ids = list(batch_df.loc[batch_df['Experimental']==1]['SampleID'])\n",
    "smri_train_np = smri_df.loc[smri_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:]\n",
    "smri_train_np = smri_train_np.astype('float64')\n",
    "np.isnan(smri_train_np)\n",
    "smri_train_np[np.isnan(smri_train_np)]=1.0\n",
    "ocd_train_np = np.concatenate(ocd_df.loc[ocd_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:])\n",
    "train_y_prob=[]\n",
    "for i in range(len(que_train_np)):\n",
    "  train_y_prob.append(1/(1+np.exp(-(np.dot(smri_train_np[i],train_model_coef)+train_model_intercept))))\n",
    "smri_y_prob=np.array(train_y_prob)\n",
    "\n",
    "# dMRI\n",
    "train_model_coef = np.mean(dmri_coef, axis=0)\n",
    "train_model_intercept = np.mean(dmri_intercept)\n",
    "\n",
    "sample_ids = list(batch_df.loc[batch_df['Experimental']==1]['SampleID'])\n",
    "dmri_train_np = dmri_df.loc[dmri_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:]\n",
    "dmri_train_np = dmri_train_np.astype('float64')\n",
    "np.isnan(dmri_train_np)\n",
    "dmri_train_np[np.isnan(dmri_train_np)]=1.0\n",
    "ocd_train_np = np.concatenate(ocd_df.loc[ocd_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:])\n",
    "train_y_prob=[]\n",
    "for i in range(len(que_train_np)):\n",
    "  train_y_prob.append(1/(1+np.exp(-(np.dot(dmri_train_np[i],train_model_coef)+train_model_intercept))))\n",
    "dmri_y_prob=np.array(train_y_prob)\n",
    "\n",
    "# que\n",
    "train_model_coef = np.mean(que_coef, axis=0)\n",
    "train_model_intercept = np.mean(que_intercept)\n",
    "\n",
    "sample_ids = list(batch_df.loc[batch_df['Experimental']==1]['SampleID'])\n",
    "que_train_np = que_df.loc[que_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:]\n",
    "que_train_np = que_train_np.astype('float64')\n",
    "np.isnan(que_train_np)\n",
    "que_train_np[np.isnan(que_train_np)]=1.0\n",
    "ocd_train_np = np.concatenate(ocd_df.loc[ocd_df['SampleID'].isin(sample_ids)].to_numpy()[:,1:])\n",
    "train_y_prob=[]\n",
    "for i in range(len(que_train_np)):\n",
    "  train_y_prob.append(1/(1+np.exp(-(np.dot(que_train_np[i],train_model_coef)+train_model_intercept))))\n",
    "que_y_prob=np.array(train_y_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052a0e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale Probabilities\n",
    "wgen_y_prob = gen_y_prob * w_gen\n",
    "wsmri_y_prob = smri_y_prob * w_smri\n",
    "wdmri_y_prob = dmri_y_prob * w_dmri\n",
    "wque_y_prob = que_y_prob * w_que\n",
    "\n",
    "total_y_prob = wgen_y_prob + wsmri_y_prob + wdmri_y_prob + wque_y_prob\n",
    "\n",
    "print(total_y_prob)\n",
    "\n",
    "total_y_prob[total_y_prob>0.5]=1\n",
    "total_y_prob[total_y_prob<=0.5]=0\n",
    "total_y_prob = total_y_prob.astype('int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241786cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine SampleIDs \n",
    "\n",
    "PredAmbig = list(zip(sample_ids, total_y_prob))\n",
    "\n",
    "df = pd.DataFrame(PredAmbig, columns = [\"SampleID\", \"OCD\"])\n",
    "\n",
    "df.to_csv('PredAmbig.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5074a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "TP=1\n",
    "FN=1\n",
    "FP=1\n",
    "TF=1\n",
    "\n",
    "P = TP + FN\n",
    "N = FP + TN\n",
    "PP = TP + FP\n",
    "PN = FN + TN\n",
    "\n",
    "print(\"Population =\", P+N)\n",
    "print(\"Prevalence =\", P/(P+N))\n",
    "\n",
    "CK = (2*((TP * TN) - (TN * FP)))/(((TP + FP) * (FP + TN)) + ((TP + FN) * (FN+TN)))\n",
    "ACC = (TP + TN) / (P + N)\n",
    "PPV = TP / PP    \n",
    "FOR = FN / PN\n",
    "FDR = FP / PP\n",
    "NPV = TN / PN\n",
    "TPR = TP / P\n",
    "FPR = FP / N\n",
    "FNR = FN / P\n",
    "TNR = TN / N\n",
    "LRp = TPR / FPR\n",
    "LRn = FNR / TNR\n",
    "MK = PPV + NPV - 1\n",
    "BM = TPR + TNR - 1\n",
    "PT = (sqrt(TPR+FPR) - FPR) / (TPR - FPR)\n",
    "DOR = LRp/LRn\n",
    "BA = (TPR + TNR) / 2\n",
    "FS = (2*PPV * TPR) / (PPV + TPR)\n",
    "FM = sqrt(PPV * TPR)\n",
    "MCC = sqrt(TPR*TNR*PPV*NPV) - sqrt(FPR*FNR*FDR*FOR)\n",
    "TS = TP / (TP + FN + FP)\n",
    "       \n",
    "print(CK)\n",
    "print(ACC)\n",
    "print(PPV)\n",
    "print(FOR)\n",
    "print(FDR)\n",
    "print(NPV)\n",
    "print(TPR)\n",
    "print(FPR)\n",
    "print(TNR)\n",
    "print(FNR)\n",
    "print(LRp)\n",
    "print(LRn)\n",
    "print(MK)\n",
    "print(BM)\n",
    "print(PT)\n",
    "print(DOR)\n",
    "print(BA)\n",
    "print(FS)\n",
    "print(FM)\n",
    "print(MCC)\n",
    "print(TS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c668771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
